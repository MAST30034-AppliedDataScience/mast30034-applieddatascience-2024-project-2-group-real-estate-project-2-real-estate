{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import json\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for JSON files \n",
    "landing_base_directory = \"../../data/landing/domain_data\"\n",
    "\n",
    "# if you want to use the newest domain data to proceed, uncomment the line below and comment the above line.\n",
    "# landing_base_directory = \"../../data/landing/domain_data_new\"\n",
    "# Remember: using newest domain data will have different result from what we had in presentation and summary notebook!\n",
    "\n",
    "# This function reads a JSON file from the provided file name and returns the data.\n",
    "def read_json_file(file_name):\n",
    "    # Construct the full file path using the base directory\n",
    "    file_path = os.path.join(landing_base_directory, file_name)\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../../data/raw/domain_data' already exists.\n"
     ]
    }
   ],
   "source": [
    "# This block checks if a folder exists at the specified path.\n",
    "\n",
    "folder_path = '../../data/raw/domain_data'\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{folder_path}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "house = read_json_file(\"house.json\")\n",
    "apartment = read_json_file(\"apartment.json\")\n",
    "town_house = read_json_file(\"town_house.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criminal_file = '../../data/landing/other_data/Data_Tables_LGA_Criminal_Incidents_Year_Ending_March_2024.xlsx'\n",
    "criminal_data = pd.read_excel(criminal_file, sheet_name='Table 03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_region = pd.read_csv('../../data/landing/region_data/key_statistics/all_region_key_data.csv')\n",
    "# if you want to use the newest ABS region data to proceed, uncomment the line below and comment the above line.\n",
    "# data_by_region = pd.read_csv('../../data/landing/region_data/key_statistics/all_region_key_data_new.csv')\n",
    "# Remember: using newest ABS data will have different result from what we had in presentation and summary notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts data from JSON format into a flattened CSV structure. \n",
    "def convert_json_to_csv(data, json_file):\n",
    "    flat_data = []\n",
    "    directory = '../../data/raw/domain_data'\n",
    "\n",
    "    # It loops through each property in the JSON, extracts key details \n",
    "    for property_url, details in data.items():\n",
    "\n",
    "        bed_info = next((room for room in details['rooms'] if 'Bed' in room), 'N/A')\n",
    "        bath_info = next((room for room in details['rooms'] if 'Bath' in room), 'N/A')\n",
    "\n",
    "        flat_data.append({\n",
    "            'property_url': property_url,\n",
    "            'name': details.get('name', 'N/A'),\n",
    "            'property_type': details.get('property_type', 'N/A'),\n",
    "            'cost_text': details.get('cost_text', 'N/A'),\n",
    "            'latitude': details.get('latitude', 'N/A'),\n",
    "            'longitude': details.get('longitude', 'N/A'),\n",
    "            'bed_info': bed_info,\n",
    "            'bath_info': bath_info,\n",
    "            'parking': details.get('parking', 'N/A'),\n",
    "            'date_available': details.get('date_available', 'N/A'),\n",
    "            'desc': details.get('desc', '').strip('</')  \n",
    "        })\n",
    "\n",
    "    # saved as a CSV file\n",
    "    property_df = pd.DataFrame(flat_data)\n",
    "    file_path = os.path.join(directory, json_file)\n",
    "    property_df.to_csv(file_path, index=False)\n",
    "    print(f\"Data successfully converted to CSV and saved at {file_path}\")\n",
    "\n",
    "    return property_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully converted to CSV and saved at ../../data/raw/domain_data/house.csv\n",
      "Data successfully converted to CSV and saved at ../../data/raw/domain_data/apartment.csv\n",
      "Data successfully converted to CSV and saved at ../../data/raw/domain_data/town_house.csv\n"
     ]
    }
   ],
   "source": [
    "house_df = convert_json_to_csv(house, \"house.csv\")\n",
    "apartment_df = convert_json_to_csv(apartment, \"apartment.csv\")\n",
    "town_house_df = convert_json_to_csv(town_house, \"town_house.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block merges the DataFrames for town houses, houses, and apartments into a single DataFrame \n",
    "merged_data = pd.concat([town_house_df, house_df, apartment_df], ignore_index=True)\n",
    "merged_data.to_csv('../../data/raw/domain_data/properties_data1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get post code\n",
    "merged_data['post_code'] = merged_data['name'].str.extract(r'(\\d{4})$')\n",
    "\n",
    "# Get region \n",
    "merged_data['region'] = merged_data['name'].str.extract(r',\\s*([^,0-9]+)\\s+\\d{4}$')\n",
    "merged_data['region'] = merged_data['region'].str.replace(r'\\s*VIC\\s*$', '', regex=True)\n",
    "\n",
    "# Get numbers from the bed and bath columns\n",
    "merged_data['bed_info'] = merged_data['bed_info'].str.extract(r'(\\d+)').astype(int)\n",
    "merged_data['bath_info'] = merged_data['bath_info'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "# Get the number from the parking column and replace N/A with 0\n",
    "merged_data['parking'] = merged_data['parking'].str.extract(r'(\\d+)')\n",
    "merged_data['parking'] = merged_data['parking'].fillna(0).astype(int)\n",
    "\n",
    "# Get the numbers in the rent, remove non-numeric characters like \"$\" and \"weekly\", and convert them to floating-point numbers\n",
    "merged_data['cost_text'] = merged_data['cost_text'].str.extract(r'([\\d,\\.]+)').replace(',', '', regex=True).astype(float)\n",
    "\n",
    "# Take the data within the reasonable rent range\n",
    "merged_data = merged_data[merged_data['cost_text'].between(10, 5000)]\n",
    "\n",
    "merged_data = merged_data.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('../../data/raw/domain_data/properties_data2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proprocessing ABS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the '-' symbol from the Region column\n",
    "data_by_region['Region'] = data_by_region['Region'].str.replace('-', '', regex=False)\n",
    "data_by_region['Region'] = data_by_region['Region'].str.replace('(', '', regex=False)\n",
    "data_by_region['Region'] = data_by_region['Region'].str.replace(')', '', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep columns that are helpful in predicting house prices\n",
    "columns_to_keep = [\n",
    "    'Region',\n",
    "    'Children enrolled in a preschool or preschool program (no.)',\n",
    "    'Estimated resident population (no.)',\n",
    "    'Land area (ha)',\n",
    "    'Median monthly household mortgage payment ($)',\n",
    "    'Median price of established house transfers ($)',\n",
    "    'Median total income (excl. Government pensions and allowances) ($)',\n",
    "    'Median weekly household rental payment ($)',\n",
    "    'Number of jobs',\n",
    "    'Working age population (aged 15-64 years) (%)'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric values\n",
    "# Converts the strings to numeric values\n",
    "# NaN values are then filled with the column's median\n",
    "data_by_region = data_by_region[columns_to_keep]\n",
    "\n",
    "for col in data_by_region.columns:\n",
    "    if col != 'Region':\n",
    "        data_by_region[col] = data_by_region[col].str.split('/').str[-1].str.strip()\n",
    "\n",
    "        data_by_region[col] = data_by_region[col].str.replace(' ', '')\n",
    "    \n",
    "        data_by_region[col] = pd.to_numeric(data_by_region[col], errors='coerce')\n",
    "        data_by_region[col].fillna(data_by_region[col].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_cleaned = merged_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions that is present in data_by_region but not in merged_data_cleaned: {'East Bendigo  Kennington', 'Benalla Surrounds', 'Wilsons Promontory', 'Bentleigh East  South', 'Dandenong  South', 'Corangamite  South', 'Coburg  East', 'Point Lonsdale  Queenscliff', 'Taylors Lakes', 'Daylesford', 'Lilydale  Coldstream', 'Warrnambool  North', 'West Melbourne  Residential', 'Knoxfield  Scoresby', 'Northcote  East', 'Ivanhoe East  Eaglemont', 'Werribee  South', 'Truganina  North', 'Clifton Hill  Alphington', 'Plenty  Yarrambat', 'Geelong West  Hamlyn Heights', 'Beaconsfield  Officer', 'Red Cliffs', 'Glenroy  West', 'Corio  Lovely Banks', 'Mornington  West', 'Bannockburn', 'Belgrave  Selby', 'Sandringham  Black Rock', 'Dromana', 'Berwick  South West', 'Merbein', 'Morwell', 'Heidelberg  Rosanna', 'Newcomb  Moolap', 'Craigieburn  Central', 'Cranbourne East  North', 'St Kilda  West', 'Seaford Vic.', 'Glenroy  East', 'Otway', 'Essendon Airport', 'Mount Waverley  North', 'Shepparton  South East', 'Rosebud  McCrae', 'Clifton Springs', 'Carrum  Patterson Lakes', 'Mernda  North', 'Mitcham Vic.', 'Epping  South', 'Preston  East', 'Bundoora  East', 'Epping Vic.  West', 'Tarneit  Central', 'Bundoora  West', 'Bunyip  Garfield', 'Kingsbury', 'Maffra', 'Ararat Surrounds', 'Edithvale  Aspendale', 'Craigieburn  South', 'Werribee  East', 'Yallourn North  Glengarry', 'Upwey  Tecoma', 'Roxburgh Park  North', 'Gordon Vic.', 'Yarra Valley', 'Wyndham Vale  North', 'Southern Grampians', 'Ardeer  Albion', 'Mildura Surrounds', 'Wonthaggi  Inverloch', 'Alexandra', 'Wattle Glen  Diamond Creek', 'Narre Warren  South West', 'Warrnambool  South', 'Moe  Newborough', 'Rowville  South', 'Surrey Hills West  Canterbury', 'St Arnaud', 'Brunswick  South', 'Warrandyte  Wonga Park', 'Wangaratta Surrounds', 'Delahey', 'Ashwood  Chadstone', 'Creswick  Clunes', 'Towong', 'Ferntree Gully  North', 'Skye  Sandhurst', 'Gowanbrae', 'Clyde North  South', 'Kinglake', 'Maryborough Surrounds', 'Sebastopol  Redan', 'Melbourne CBD  West', 'Norlane', 'Kensington Vic.', 'Southbank West  South Wharf', 'Cranbourne South', 'Caulfield  South', 'Chelsea  Bonbeach', 'Ballarat North  Invermay', 'Cranbourne North  West', 'Cranbourne North  East', 'Pearcedale  Tooradin', 'Caulfield  North', 'Ararat', 'Croydon  East', 'Wandin  Seville', 'Moyne  West', 'Lynbrook  Lyndhurst', 'Romsey', 'Hoppers Crossing  North', 'Yarriambiack', 'Manor Lakes  Quandong', 'Alps  East', 'Viewbank  Yallambie', 'Hastings  Somers', 'Rutherglen', 'Point Cook  East', 'Research  North Warrandyte', 'Hampton Park  East', 'Narre Warren North', 'Horsham Surrounds', 'Baranduda  Leneva', 'Beaufort', 'Seddon  Kingsville', 'Hurstbridge', 'Bright  Mount Beauty', 'Mansfield Vic.', 'Heathcote', 'Smythes Creek', 'Strathfieldsaye', 'Burwood Vic.', 'Newtown Vic.', 'Pakenham  North East', 'Roxburgh Park South  Somerton', 'Bendigo Surrounds  North', 'Bacchus Marsh Surrounds', 'California Gully  Eaglehawk', 'Greenvale  Bulla', 'Clayton North  Notting Hill', 'Kangaroo Flat  Golden Square', 'Yarram', 'Wyndham Vale  South', 'Swan Hill Surrounds', 'Endeavour Hills  South', 'Cranbourne East  South', 'Hampton Park  West', 'Korumburra', 'Panton Hill  St Andrews', 'Lysterfield', 'Prahran  Windsor', 'Mill Park  North', 'Kurunjang  Toolern Vale', 'Hoppers Crossing  South', 'West Wimmera', 'Doreen  South', 'Seymour', 'Paynesville', 'Narre Warren  North East', 'Craigieburn  North', 'Noble Park North', 'French Island', 'Reservoir  South East', 'Corangamite  North', 'Mildura  South', 'Woodend', 'Rushworth', 'Pakenham  South West', 'Mill Park  South', 'Maryborough Vic.', 'Ballarat', 'Kew  West', 'Monbulk  Silvan', 'Kilmore  Broadford', 'Rosedale', 'Niddrie  Essendon West', 'Doveton', 'Macedon', 'Preston  West', 'Barwon Heads  Armstrong Creek', 'Glen Iris  East', 'Noble Park  West', 'Ferntree Gully South  Upper Ferntree Gully', 'Truganina  South East', 'West Footscray  Tottenham', 'Pakenham  North West', 'Echuca', 'Essendon  East', 'Epping  East', 'Longford  Loch Sport', 'Rockbank  Mount Cottrell', 'Maiden Gully', 'Tarneit West  Mount Cottrell', 'Narre Warren South  West', 'Phillip Island', 'Donvale  Park Orchards', 'Trafalgar Vic.', 'Ashburton Vic.', 'Mount Dandenong  Olinda', 'Noble Park  East', 'Berwick  South East', 'Bruthen  Omeo', 'Emerald  Cockatoo', 'Yea', 'Swan Hill', 'Bentleigh  McKinnon', 'Reservoir  South West', 'Richmond South  Cremorne', 'Golden Plains  South', 'Bendigo Surrounds  South', 'Euroa', 'South Morang  South', 'Alphington  Fairfield', 'Royal Botanic Gardens Victoria', 'Burnside', 'Orbost', 'Aspendale Gardens  Waterways', 'Campbellfield  Coolaroo', 'Golden Plains  North', 'Shepparton Surrounds  East', 'Sale', 'Truganina  South West', 'Traralgon  East', 'St Kilda  Central', 'Buloke', 'Stawell', 'Bundoora  North', 'Lorne  Anglesea', 'Tarneit  South', 'Clyde North  North', 'Dandenong  North', 'Horsham', 'Rochester', 'Montmorency  Briar Hill', 'Grovedale  Mount Duneed', 'Mordialloc  Parkdale', 'Glen Waverley  West', 'Kings Park Vic.', 'Endeavour Hills  North', 'Reservoir  North East', 'Mernda  South', 'Canadian  Mount Clear', 'Doncaster East  South', 'Lalor  West', 'Flora Hill  Spring Gully', 'Mornington  East', 'Northcote  West', 'Essendon West  Aberfeldie', 'Fraser Rise  Plumpton', 'Shepparton Surrounds  West', 'The Basin', 'Castlemaine', 'Point Cook  North East', 'Yackandandah', 'Traralgon  West', 'Pakenham  South East', 'Castlemaine Surrounds', 'Brighton Vic.', 'Flemington Racecourse', 'Benalla', 'North Geelong  Bell Park', 'Kerang', 'Avoca', 'Ballarat East  Warrenheip', 'Port Melbourne Industrial', 'South Yarra  West', 'Glen Waverley  East', 'Clayton  Central', 'Lockington  Gunbower', 'White Hills  Ascot', 'West Melbourne  Industrial', 'Tarneit  North', 'Melbourne CBD  East', 'South Morang  North', 'Kew  South', 'Keysborough  North', 'Point Nepean', 'Richmond  North', 'Wendouree  Miners Rest', 'Wangaratta', 'Craigieburn  North West', 'Werribee  West', 'Rowville  Central', 'Oakleigh  Huntingdale', 'Winchelsea', 'Loddon', 'Gladstone Park  Westmeadows', 'Moyne  East', 'Reservoir  North West', 'Brunswick  North', 'Moorabbin  Heatherton', 'Healesville  Yarra Glen', 'Colac Surrounds', 'South Yarra  North', 'Foster', 'Point Cook  South', 'Keysborough  South', 'Lalor  East', 'Mount Waverley  South', 'Nhill Region', 'Highett West  Cheltenham', 'Bentleigh East  North', 'Croydon  West', 'Rowville  North', 'Point Cook  North West', 'Upper Yarra Valley', 'St Albans  South', 'Southbank  East', 'Malvern  Glen Iris', 'Hawthorn  South', 'Hamilton Vic.', 'Gannawarra', 'Ormond  Glen Huntly', 'Shepparton  North', 'Melton South  Weir Views', 'Lake King', 'Carlton North  Princes Hill', 'Eynesbury  Exford', 'Moorabbin Airport', 'Melton', 'Charlemont', 'Irymple', 'Hawthorn  North', 'Chiltern  Indigo Valley', 'Mildura  North', 'Doncaster East  North', 'Leongatha', 'Croydon Hills  Warranwood', 'Robinvale', 'Narre Warren South  East', 'Mooroopna', 'Melbourne CBD  North', 'St Albans  North', 'Melbourne Airport', 'Sunbury  South', 'Seymour Surrounds', 'Kyneton', 'Surrey Hills East  Mont Albert', 'Montrose', 'Altona Meadows', 'Somerville', 'South Yarra  South', 'Cobblebank  Strathtulloh', 'Nagambie', 'Berwick  North', 'Craigieburn  West', 'Moira', 'Mickleham  Yuroke', 'Mount Baw Baw Region', 'Alps  West', 'Doreen  North', 'Bairnsdale', 'Coburg  West', 'Glenelg Vic.', 'Koo Wee Rup', 'Braeside', 'Highett East  Cheltenham ', 'Clarinda  Oakleigh South', 'Sunbury  West'}\n",
      "Region that is cleaned in merged_data_cleaned but not in data_by_region: {'Merricks Beach', 'Brighton', 'Essendon', 'Newcomb', 'Deanside', 'Box Hill South', 'Princes Hill', 'Beveridge', 'East Geelong', 'Melton South', 'Glen Iris', 'Ballan', 'Moorabbin', 'Warrnambool', 'Marshall', 'Kensington', 'Tarneit', 'Aintree', 'Mitcham', 'Leneva', 'Thomson', 'Manor Lakes', 'Doncaster East', 'Noble Park', 'Lucas', 'Bonnie Brook', 'Cranbourne North', 'Scoresby', 'Huntingdale', 'Hawthorn', 'Killara', 'Rowville', 'Curlewis', 'West Melbourne', 'Ferntree Gully', 'Redan', 'Kooyong', 'Ashburton', 'Prahran', 'Highett', 'Middle Park', 'Bundoora', 'Endeavour Hills', 'Doreen', 'Kennington', 'Truganina', 'Ashwood', 'Kew', 'Kingsville', 'Chelsea', 'Knoxfield', 'Somers', 'Diamond Creek', 'Williamstown North', 'Deepdene', 'Notting Hill', 'Grovedale', 'Armstrong Creek', 'Carrum', 'Herne Hill', 'Mont Albert North', 'Donnybrook', 'Greenvale', 'Mill Park', 'Port Fairy', 'St Leonards', 'Ballarat Central', 'Coburg', 'Ripponlea', 'Lynbrook', 'Beaconsfield', 'Parkdale', 'Hoppers Crossing', 'Cobden', 'Canterbury', 'Craigieburn', 'Burnley', 'Glen Huntly', 'Bentleigh East', 'Watsonia North', 'Timboon', 'Werribee South', 'Heathmont', 'Kangaroo Flat', 'Heidelberg Heights', 'Tallangatta', 'Burwood', 'Kilsyth South', 'Croydon Hills', 'Ovens', 'Hampton East', 'Lake Wendouree', 'Caulfield North', 'Portsea', 'Ardeer', 'Baranduda', 'Brooklyn', 'Traralgon', 'Heidelberg', 'Black Rock', 'Croydon North', 'Aspendale Gardens', 'Surrey Hills', 'Mount Waverley', 'Albion', 'Rippleside', 'Quarry Hill', 'Mordialloc', 'Mildura', 'Maidstone', 'Glen Waverley', 'Barwon Heads', 'Strathtulloh', 'McCrae', 'Sandringham', 'Seddon', 'Essendon North', 'Mambourin', 'Brunswick', 'Winter Valley', 'Mount Duneed', 'Safety Beach', 'Southbank', 'Peterborough', 'Point Cook', 'Bell Park', 'Clyde North', 'Fairfield', 'Moe', 'Shepparton', 'Bonbeach', 'Sandhurst', 'Berwick', 'Oakleigh', 'Lower Plenty', 'Pakenham', 'Whittington', 'Warranwood', 'Black Hill', 'Officer', 'South Geelong', 'New Gisborne', 'Caulfield South', 'Elliminyt', 'Cobblebank', 'Carlton North', 'West Footscray', 'Bellfield', 'Lyndhurst', 'St Albans Park', 'Wyndham Vale', 'Canadian', 'Caulfield', 'Essendon West', 'St Helena', 'Glenroy', 'Baxter', 'Williams Landing', 'Seaford', 'Cremorne', 'Preston', 'Mornington', 'Mont Albert', 'California Gully', 'South Yarra', 'Aberfeldie', 'Windsor', 'South Kingsville', 'Hampton Park', 'Rosanna', 'Ivanhoe East', 'Clyde', 'Patterson Lakes', 'Spotswood', 'Chadstone', 'Tyabb', 'Richmond', 'Sorrento', 'Bell Post Hill', 'Lilydale', 'Ormond', 'Fraser Rise', 'Newington', 'Roxburgh Park', 'Kalkallo', 'Alphington', 'Tootgarook', 'Donvale', 'Rockbank', 'Niddrie', 'Rye', 'Oakleigh South', 'Newtown', 'Clayton', 'St Kilda West', 'South Morang', 'Wendouree', 'Weir Views', 'Soldiers Hill', 'Croydon', 'Narre Warren South', 'Gardenvale', 'Nar Nar Goon', 'Harkness', 'Bright', 'Eynesbury', 'Narre Warren', 'Dandenong', 'St Albans', 'Shoreham', 'Ballarat East', 'Geelong West', 'Cheltenham', 'McKinnon', 'Clifton Hill', 'Albanvale', 'Mirboo North', 'Malvern', 'Blairgowrie', 'Heatherton', 'Wonthaggi', 'Reservoir', 'Golden Point', 'Bonshaw', 'Gladstone Park', 'Aspendale', 'Werribee', 'Bentleigh', 'Edithvale', 'Epping', 'Melbourne', 'Macleod', 'St Kilda', 'Lalor', 'Northcote', 'Queenscliff', 'Golden Square', 'Darley', 'Clarinda', 'Longlea', 'Kurunjang', 'Oakleigh East', 'Balaclava', 'Keysborough', 'Mernda', 'Thornhill Park', 'Hamlyn Heights', 'Mickleham', 'Corio', 'Cranbourne East'}\n"
     ]
    }
   ],
   "source": [
    "data_by_region_unique_regions = set(data_by_region['Region'].unique())\n",
    "merged_data_cleaned_unique_regions = set(merged_data_cleaned['region'].unique())\n",
    "\n",
    "# Find the difference\n",
    "diff_in_data_by_region = data_by_region_unique_regions - merged_data_cleaned_unique_regions\n",
    "diff_in_merged_data_cleaned = merged_data_cleaned_unique_regions - data_by_region_unique_regions\n",
    "\n",
    "print(f\"Regions that is present in data_by_region but not in merged_data_cleaned: {diff_in_data_by_region}\")\n",
    "print(f\"Region that is cleaned in merged_data_cleaned but not in data_by_region: {diff_in_merged_data_cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the closest matching region name from a list of cleaned region names.\n",
    "def get_closest_match(region, cleaned_regions):\n",
    "    match, score = process.extractOne(region, cleaned_regions)\n",
    "    if score > 80:  \n",
    "        return match\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmatched region: ['Smythes Creek' 'Creswick  Clunes' 'Daylesford' 'Gordon Vic.' 'Avoca'\n",
      " 'Beaufort' 'Maryborough Vic.' 'Maryborough Surrounds' 'Maiden Gully'\n",
      " 'Strathfieldsaye' 'Castlemaine' 'Castlemaine Surrounds' 'Heathcote'\n",
      " 'Kyneton' 'Woodend' 'Loddon' 'Bannockburn' 'Charlemont' 'Norlane'\n",
      " 'Clifton Springs' 'Lorne  Anglesea' 'Alexandra' 'Euroa'\n",
      " 'Kilmore  Broadford' 'Mansfield Vic.' 'Nagambie' 'Seymour'\n",
      " 'Seymour Surrounds' 'Yea' 'Benalla' 'Benalla Surrounds' 'Rutherglen'\n",
      " 'Wangaratta' 'Wangaratta Surrounds' 'Towong' 'Yackandandah'\n",
      " 'Trafalgar Vic.' 'Bairnsdale' 'Bruthen  Omeo' 'Orbost' 'Paynesville'\n",
      " 'Foster' 'French Island' 'Korumburra' 'Leongatha' 'Phillip Island'\n",
      " 'Wilsons Promontory' 'Morwell' 'Longford  Loch Sport' 'Maffra' 'Sale'\n",
      " 'Braeside' 'Viewbank  Yallambie' 'Kingsbury' 'Hurstbridge' 'Kinglake'\n",
      " 'Plenty  Yarrambat' 'Macedon' 'Romsey' 'Gowanbrae' 'Lysterfield'\n",
      " 'The Basin' 'Belgrave  Selby' 'Monbulk  Silvan' 'Montrose'\n",
      " 'Upwey  Tecoma' 'Wandin  Seville' 'Emerald  Cockatoo' 'Bunyip  Garfield'\n",
      " 'Koo Wee Rup' 'Doveton' 'Pearcedale  Tooradin' 'Delahey' 'Taylors Lakes'\n",
      " 'Dromana' 'Point Nepean' 'Somerville' 'Ararat' 'Ararat Surrounds'\n",
      " 'Horsham' 'Horsham Surrounds' 'Nhill Region' 'Stawell' 'West Wimmera'\n",
      " 'Yarriambiack' 'Irymple' 'Merbein' 'Red Cliffs' 'Buloke' 'Gannawarra'\n",
      " 'Kerang' 'Robinvale' 'Echuca' 'Lockington  Gunbower' 'Rochester'\n",
      " 'Rushworth' 'Moira' 'Mooroopna' 'Glenelg Vic.' 'Hamilton Vic.'\n",
      " 'Southern Grampians' 'Otway' 'Moyne  East' 'Moyne  West']\n"
     ]
    }
   ],
   "source": [
    "# This block applies the 'get_closest_match' function to the 'Region' column in 'data_by_region' \n",
    "data_by_region['matched_region'] = data_by_region['Region'].apply(lambda x: get_closest_match(x, merged_data_cleaned['region'].unique()))\n",
    "\n",
    "unmatched_data = data_by_region[data_by_region['matched_region'].isna()]\n",
    "print(f\"unmatched region: {unmatched_data['Region'].unique()}\")\n",
    "\n",
    "merged_final_geo = pd.merge(data_by_region, merged_data_cleaned, left_on='matched_region', right_on='region', how='left')\n",
    "merged_final_geo.drop(columns=['matched_region'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_cleaned['region'] = merged_data_cleaned['region'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The areas that not successfully matched: ['Tallangatta' 'Aintree' 'Spotswood' 'Maidstone' 'Macleod' 'Baxter'\n",
      " 'Brooklyn' 'Rippleside' 'Williams Landing' 'Whittington' 'Deanside'\n",
      " 'Marshall' 'Beveridge' 'Safety Beach' 'Darley' 'Tyabb' 'Kalkallo'\n",
      " 'Newington' 'Donnybrook' 'Heathmont' 'Tootgarook' 'Balaclava' 'Curlewis'\n",
      " 'Mambourin' 'Portsea' 'Shoreham' 'Rye' 'Burnley' 'Merricks Beach'\n",
      " 'Kooyong' 'Blairgowrie' 'Cobden' 'Elliminyt' 'Bonnie Brook'\n",
      " 'Nar Nar Goon' 'Harkness' 'Ovens' 'Peterborough' 'Longlea' 'Albanvale'\n",
      " 'Timboon' 'Bonshaw' 'Sorrento' 'Lucas' 'Ripponlea' 'Gardenvale'\n",
      " 'Lower Plenty' 'Deepdene']\n"
     ]
    }
   ],
   "source": [
    "# Adds a new column for merged_data_cleaned, storing the fuzzy matched region name\n",
    "merged_data_cleaned['matched_region'] = merged_data_cleaned['region'].apply(lambda x: get_closest_match(x, data_by_region['Region'].unique()))\n",
    "\n",
    "# View data that is not successfully matched\n",
    "unmatched_data = merged_data_cleaned[merged_data_cleaned['matched_region'].isna()]\n",
    "print(f\"The areas that not successfully matched: {unmatched_data['region'].unique()}\")\n",
    "\n",
    "# Merge two data sets using fuzzy matched region names\n",
    "merged_final = pd.merge(merged_data_cleaned, data_by_region, left_on='matched_region', right_on='Region', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final_geo.to_csv('../../data/raw/domain_data/merged_final_geo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Criminal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep column we need\n",
    "columns_to_keep = ['Year', 'Year ending', 'Local Government Area', 'Postcode', 'Suburb/Town Name', 'Offence Division']\n",
    "criminal_data = criminal_data[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_filtered = criminal_data[criminal_data['Year'].between(2020, 2024)]\n",
    "\n",
    "# Calculate the annual number of crimes based on Postcode\n",
    "crime_avg_by_postcode = crime_filtered.groupby('Postcode').size().div(5).reset_index(name='avg_crime_count')\n",
    "\n",
    "merged_final['post_code'] = merged_final['post_code'].astype(str)\n",
    "crime_avg_by_postcode['Postcode'] = crime_avg_by_postcode['Postcode'].astype(str)\n",
    "\n",
    "# Merge the average annual crime count into merged_data_cleaned\n",
    "merged_final = pd.merge(merged_final, crime_avg_by_postcode, left_on='post_code', right_on='Postcode', how='left')\n",
    "\n",
    "# Delete redundant Postcode columns\n",
    "merged_final.drop('Postcode', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958, 26)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "folder_path = '../../data/raw/other_data'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Save the CSV file to the newly created folder\n",
    "criminal_data.to_csv(f'{folder_path}/criminal_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types and their column counts:\n",
      "float64    13\n",
      "object     10\n",
      "int64       3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "type_counts = merged_final.dtypes.value_counts()\n",
    "print(\"Data types and their column counts:\")\n",
    "print(type_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories for each column in columns_to_modify:\n",
      "property_type      8\n",
      "Region           291\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "columns_to_modify = ['property_type', 'Region']\n",
    "category_counts = merged_final[columns_to_modify].nunique()\n",
    "print(\"Number of unique categories for each column in columns_to_modify:\")\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding for region\n",
    "label_encoder = LabelEncoder()\n",
    "merged_final['region_encoded'] = label_encoder.fit_transform(merged_final['Region'])\n",
    "merged_final = merged_final.drop(columns=[\"Region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for property_type\n",
    "merged_final = pd.get_dummies(merged_final, columns=['property_type'], prefix='property_type', drop_first=True)\n",
    "property_type_columns = [col for col in merged_final.columns if 'property_type_' in col]\n",
    "merged_final[property_type_columns] = merged_final[property_type_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach \n",
    "# diff_property_url_in_merged = ~merged_final['property_url'].isin(property_df0['property_url'])\n",
    "# diff_index_in_merged = merged_final[diff_property_url_in_merged].index\n",
    "# merged_final_test_cleaned = merged_final.drop(diff_index_in_merged)\n",
    "# merged_final_test_cleaned_sorted = merged_final_test_cleaned.set_index('property_url').reindex(property_df0['property_url']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final = merged_final.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2843, 32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final.to_csv('../../data/curated/properties_data3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
